{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latin Master Bibliography Data Exploration\n",
    "\n",
    "This notebook provides tools for exploring and analyzing the Latin Master Bibliography dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataset contains deduplicated Latin printed works from 1450-1900, combining data from multiple major bibliographic catalogues:\n",
    "- USTC (Universal Short Title Catalogue)\n",
    "- VD16/VD17/VD18 (German printing catalogues)\n",
    "- ESTC (English Short Title Catalogue)\n",
    "\n",
    "Each row represents a unique Latin work/edition with information about which catalogues attest it and any available digital facsimiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main dataset\n",
    "data_path = Path('../data/processed/final/latin_master_bibliography.csv')\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(data_path, encoding='utf-8-sig')\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Dataset not found at {data_path}\")\n",
    "    print(\"Please run the pipeline first to generate the dataset.\")\n",
    "    # Create a sample dataset for demonstration\n",
    "    df = pd.DataFrame({\n",
    "        'title': ['De Revolutionibus Orbium Coelestium', 'Ars Nova', 'Summa Theologica'],\n",
    "        'author': ['Copernicus, Nicolaus', 'Anonymous', 'Aquinas, Thomas'],\n",
    "        'publication_year': [1543, 1320, 1274],\n",
    "        'publication_place': ['Nuremberg', 'Paris', 'Paris'],\n",
    "        'language': ['lat', 'lat', 'lat'],\n",
    "        'source_catalogues': ['VD16;USTC', 'USTC', 'VD16'],\n",
    "        'has_digital_facsimile': [True, False, True]\n",
    "    })\n",
    "    print(\"Created sample dataset for demonstration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 records:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics for numeric columns\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "if len(numeric_columns) > 0:\n",
    "    print(\"Numeric Columns Statistics:\")\n",
    "    display(df[numeric_columns].describe())\n",
    "else:\n",
    "    print(\"No numeric columns found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze publication dates\n",
    "if 'publication_year' in df.columns:\n",
    "    # Remove invalid years\n",
    "    valid_years = df[(df['publication_year'] >= 1450) & (df['publication_year'] <= 1900)]['publication_year']\n",
    "    \n",
    "    # Create histogram\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    # Overall distribution\n",
    "    ax1.hist(valid_years, bins=50, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    ax1.set_title('Distribution of Latin Works by Publication Year (1450-1900)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Publication Year')\n",
    "    ax1.set_ylabel('Number of Works')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Century breakdown\n",
    "    centuries = (valid_years - 1) // 100 + 1\n",
    "    century_counts = centuries.value_counts().sort_index()\n",
    "    \n",
    "    bars = ax2.bar(range(len(century_counts)), century_counts.values, \n",
    "                   color=['coral', 'lightblue', 'lightgreen', 'gold', 'plum', 'orange'][:len(century_counts)])\n",
    "    ax2.set_title('Latin Works by Century', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Century')\n",
    "    ax2.set_ylabel('Number of Works')\n",
    "    ax2.set_xticks(range(len(century_counts)))\n",
    "    ax2.set_xticklabels([f'{c}th' for c in century_counts.index])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, century_counts.values):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(century_counts.values),\n",
    "                f'{count:,}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Date Range: {valid_years.min()} - {valid_years.max()}\")\n",
    "    print(f\"Median Year: {valid_years.median():.0f}\")\n",
    "    print(f\"\\nCentury Breakdown:\")\n",
    "    for century, count in century_counts.items():\n",
    "        percentage = (count / len(valid_years)) * 100\n",
    "        print(f\"  {century}th century: {count:,} works ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"No publication_year column found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catalogue Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze catalogue coverage\n",
    "if 'source_catalogues' in df.columns:\n",
    "    # Split catalogue sources and count\n",
    "    catalogue_series = df['source_catalogues'].str.split(';').explode()\n",
    "    catalogue_counts = catalogue_series.value_counts()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Pie chart\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(catalogue_counts)))\n",
    "    wedges, texts, autotexts = ax1.pie(catalogue_counts.values, labels=catalogue_counts.index,\n",
    "                                      autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax1.set_title('Catalogue Coverage Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Bar chart\n",
    "    bars = ax2.bar(catalogue_counts.index, catalogue_counts.values, color=colors)\n",
    "    ax2.set_title('Records per Catalogue', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Catalogue')\n",
    "    ax2.set_ylabel('Number of Records')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, catalogue_counts.values):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(catalogue_counts.values),\n",
    "                f'{count:,}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(\"Catalogue Coverage Statistics:\")\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Unique catalogue mentions: {catalogue_series.nunique()}\")\n",
    "    print(f\"\\nRecords per catalogue:\")\n",
    "    for catalogue, count in catalogue_counts.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {catalogue}: {count:,} records ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Multi-catalogue overlap\n",
    "    overlap_stats = df['source_catalogues'].str.split(';').apply(len)\n",
    "    print(f\"\\nMulti-catalogue coverage:\")\n",
    "    print(f\"  Single catalogue: {(overlap_stats == 1).sum():,} records\")\n",
    "    print(f\"  Multiple catalogues: {(overlap_stats > 1).sum():,} records\")\n",
    "    print(f\"  Maximum catalogues per record: {overlap_stats.max()}\")\n",
    "else:\n",
    "    print(\"No source_catalogues column found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze publication places\n",
    "if 'publication_place' in df.columns:\n",
    "    # Get top publication places\n",
    "    place_counts = df['publication_place'].value_counts().head(20)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Horizontal bar chart\n",
    "    y_pos = range(len(place_counts))\n",
    "    bars = plt.barh(y_pos, place_counts.values, color='lightblue', edgecolor='navy')\n",
    "    \n",
    "    plt.yticks(y_pos, place_counts.index)\n",
    "    plt.xlabel('Number of Works')\n",
    "    plt.title('Top 20 Publication Places for Latin Works', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, place_counts.values)):\n",
    "        plt.text(bar.get_width() + 0.01*max(place_counts.values), bar.get_y() + bar.get_height()/2,\n",
    "                f'{count:,}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total unique publication places: {df['publication_place'].nunique():,}\")\n",
    "    print(f\"\\nTop 20 Publication Places:\")\n",
    "    for i, (place, count) in enumerate(place_counts.items(), 1):\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"  {i:2d}. {place}: {count:,} works ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"No publication_place column found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze authors\n",
    "if 'author' in df.columns:\n",
    "    # Clean author names and count\n",
    "    author_counts = df['author'].value_counts().head(20)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Horizontal bar chart\n",
    "    y_pos = range(len(author_counts))\n",
    "    bars = plt.barh(y_pos, author_counts.values, color='lightcoral', edgecolor='darkred')\n",
    "    \n",
    "    plt.yticks(y_pos, author_counts.index)\n",
    "    plt.xlabel('Number of Works')\n",
    "    plt.title('Top 20 Most Prolific Latin Authors', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, author_counts.values)):\n",
    "        plt.text(bar.get_width() + 0.01*max(author_counts.values), bar.get_y() + bar.get_height()/2,\n",
    "                f'{count}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Total unique authors: {df['author'].nunique():,}\")\n",
    "    print(f\"Works with identified authors: {df['author'].notna().sum():,} ({df['author'].notna().sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"Anonymous works: {df['author'].isna().sum():,} ({df['author'].isna().sum()/len(df)*100:.1f}%)\")\n",
    "    print(f\"\\nTop 20 Authors by Number of Works:\")\n",
    "    for i, (author, count) in enumerate(author_counts.items(), 1):\n",
    "        print(f\"  {i:2d}. {author}: {count} works\")\n",
    "else:\n",
    "    print(\"No author column found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digital Facsimile Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze digital facsimile availability\n",
    "digital_column = None\n",
    "for col in ['has_digital_facsimile', 'digital_facsimile_url', 'digital_facsimile_urls']:\n",
    "    if col in df.columns:\n",
    "        digital_column = col\n",
    "        break\n",
    "\n",
    "if digital_column:\n",
    "    if digital_column == 'has_digital_facsimile':\n",
    "        digital_counts = df[digital_column].value_counts()\n",
    "        labels = ['No Digital Facsimile', 'Has Digital Facsimile']\n",
    "        sizes = [digital_counts.get(False, 0), digital_counts.get(True, 0)]\n",
    "        colors = ['lightcoral', 'lightgreen']\n",
    "    else:\n",
    "        # Check for non-empty URLs\n",
    "        has_digital = df[digital_column].notna() & (df[digital_column] != '')\n",
    "        digital_counts = has_digital.value_counts()\n",
    "        labels = ['No Digital Facsimile', 'Has Digital Facsimile']\n",
    "        sizes = [digital_counts.get(False, 0), digital_counts.get(True, 0)]\n",
    "        colors = ['lightcoral', 'lightgreen']\n",
    "    \n",
    "    # Create pie chart\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Pie chart\n",
    "    wedges, texts, autotexts = ax1.pie(sizes, labels=labels, autopct='%1.1f%%', \n",
    "                                      colors=colors, startangle=90)\n",
    "    ax1.set_title('Digital Facsimile Availability', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Bar chart with counts\n",
    "    bars = ax2.bar(labels, sizes, color=colors)\n",
    "    ax2.set_title('Digital Facsimile Counts', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Works')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, sizes):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01*max(sizes),\n",
    "                f'{count:,}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    total_records = len(df)\n",
    "    digital_count = sizes[1] if len(sizes) > 1 else 0\n",
    "    digital_percentage = (digital_count / total_records) * 100\n",
    "    \n",
    "    print(f\"Digital Facsimile Analysis:\")\n",
    "    print(f\"  Total works: {total_records:,}\")\n",
    "    print(f\"  Works with digital facsimiles: {digital_count:,} ({digital_percentage:.1f}%)\")\n",
    "    print(f\"  Works without digital facsimiles: {total_records - digital_count:,} ({100 - digital_percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"No digital facsimile columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze titles\n",
    "if 'title' in df.columns:\n",
    "    # Title length analysis\n",
    "    title_lengths = df['title'].str.len()\n",
    "    \n",
    "    # Create word frequency analysis\n",
    "    all_titles = ' '.join(df['title'].dropna().astype(str)).lower()\n",
    "    \n",
    "    # Remove common Latin words\n",
    "    stop_words = {'de', 'der', 'die', 'das', 'des', 'dem', 'den', 'ein', 'eine', 'einer',\n",
    "                  'et', 'ad', 'in', 'a', 'ab', 'ex', 'per', 'pro', 'cum', 'sine', 'sub', \n",
    "                  'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}\n",
    "    \n",
    "    words = [word for word in all_titles.split() \n",
    "             if len(word) > 3 and word not in stop_words]\n",
    "    \n",
    "    from collections import Counter\n",
    "    word_counts = Counter(words)\n",
    "    top_words = word_counts.most_common(20)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Title length distribution\n",
    "    ax1.hist(title_lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_title('Title Length Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Number of Characters')\n",
    "    ax1.set_ylabel('Number of Works')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Most common words\n",
    "    words, counts = zip(*top_words)\n",
    "    y_pos = range(len(words))\n",
    "    bars = ax2.barh(y_pos, counts, color='lightgreen', edgecolor='darkgreen')\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(words)\n",
    "    ax2.set_xlabel('Frequency')\n",
    "    ax2.set_title('Top 20 Most Common Words in Titles', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, counts):\n",
    "        width = bar.get_width()\n",
    "        ax2.text(width + 0.01*max(counts), bar.get_y() + bar.get_height()/2,\n",
    "                f'{count}', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Title Analysis:\")\n",
    "    print(f\"  Average title length: {title_lengths.mean():.1f} characters\")\n",
    "    print(f\"  Shortest title: {title_lengths.min()} characters\")\n",
    "    print(f\"  Longest title: {title_lengths.max()} characters\")\n",
    "    print(f\"  Total unique words: {len(set(words)):,}\")\n",
    "    print(f\"\\nTop 20 Most Common Words:\")\n",
    "    for i, (word, count) in enumerate(top_words, 1):\n",
    "        print(f\"  {i:2d}. '{word}': {count} occurrences\")\n",
    "else:\n",
    "    print(\"No title column found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary report\n",
    "summary = {\n",
    "    'total_records': len(df),\n",
    "    'columns': list(df.columns),\n",
    "    'date_range': {\n",
    "        'start': df['publication_year'].min() if 'publication_year' in df.columns else 'N/A',\n",
    "        'end': df['publication_year'].max() if 'publication_year' in df.columns else 'N/A'\n",
    "    },\n",
    "    'unique_authors': df['author'].nunique() if 'author' in df.columns else 'N/A',\n",
    "    'unique_places': df['publication_place'].nunique() if 'publication_place' in df.columns else 'N/A',\n",
    "    'catalogues': df['source_catalogues'].str.split(';').explode().nunique() if 'source_catalogues' in df.columns else 'N/A'\n",
    "}\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save a sample of interesting records\n",
    "if len(df) > 0:\n",
    "    # Find records with digital facsimiles\n",
    "    digital_cols = ['has_digital_facsimile', 'digital_facsimile_url', 'digital_facsimile_urls']\n",
    "    digital_col = next((col for col in digital_cols if col in df.columns), None)\n",
    "    \n",
    "    if digital_col:\n",
    "        if digital_col == 'has_digital_facsimile':\n",
    "            digital_records = df[df[digital_col] == True]\n",
    "        else:\n",
    "            digital_records = df[df[digital_col].notna() & (df[digital_col] != '')]\n",
    "        \n",
    "        print(f\"\\nRecords with digital facsimiles: {len(digital_records)}\")\n",
    "        \n",
    "        # Save sample of digitally available works\n",
    "        if len(digital_records) > 0:\n",
    "            sample_digital = digital_records.head(10)\n",
    "            sample_file = 'sample_digital_facsimiles.csv'\n",
    "            sample_digital.to_csv(sample_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Sample of digital facsimile records saved to {sample_file}\")\n",
    "    \n",
    "    # Find works from multiple catalogues\n",
    "    if 'source_catalogues' in df.columns:\n",
    "        multi_catalogue = df[df['source_catalogues'].str.contains(';', na=False)]\n",
    "        print(f\"\\nRecords from multiple catalogues: {len(multi_catalogue)}\")\n",
    "        \n",
    "        if len(multi_catalogue) > 0:\n",
    "            sample_multi = multi_catalogue.head(10)\n",
    "            sample_file = 'sample_multi_catalogue.csv'\n",
    "            sample_multi.to_csv(sample_file, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Sample of multi-catalogue records saved to {sample_file}\")\n",
    "\n",
    "print(\"\\nExploration complete! Use the generated visualizations for further analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}